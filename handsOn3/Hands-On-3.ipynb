{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92f1ef18",
      "metadata": {
        "id": "92f1ef18"
      },
      "source": [
        "# Hands-On 3: Parallelization with MPI\n",
        "*   Enzo Bacelar Conte Gebauer\n",
        "*   Luiz Guilherme Guerreiro\n",
        "*   Maria Eduarda Lopes de Morais Brito\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01d9803-6dee-44aa-8bb1-ce51765b0616",
      "metadata": {
        "id": "e01d9803-6dee-44aa-8bb1-ce51765b0616"
      },
      "source": [
        "|  Sessions     | Codes               | files              |\n",
        "| --------------| --------------------| ------------------ |\n",
        "| Session 1     |  Basic Operations   |   operations.c   |\n",
        "| Session 2     | Algebraic Function  |  function.c      |\n",
        "| Session 3     |  Tridiagonal Matrix |   tridiagonal.c  |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a674a951",
      "metadata": {
        "id": "a674a951",
        "tags": []
      },
      "source": [
        "## `Basic Operations`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13c9861",
      "metadata": {
        "id": "b13c9861"
      },
      "source": [
        "The Algorithm below solves the multiplication, addition and subtraction of the elements of a vector of integers. The variable array is the vector on which the operations will be performed. Then, modify the program to run in parallel using MPI. Present the primitives used. The idea is made the following MPI version with only $4$ processes running.In the version, each process does a function: $1$ add, $1$ subtract and $1$ multiplies. The other process is responsible for telling each of the other $3$ its function, and when finished printing the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b72903bc-7fd4-4e56-8281-d3e3986d9d0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b72903bc-7fd4-4e56-8281-d3e3986d9d0c",
        "outputId": "01894111-11f7-4cbd-e3e1-110cc5575be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting operations.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile operations.c\n",
        "#include <stdio.h>\n",
        "#define SIZE 12\n",
        "#include <mpi.h>\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  int i, sum = 0, subtraction = 0, mult = 1;\n",
        "  int array[SIZE];\n",
        "\n",
        "  char operations[] = {'+', '-', '*'};\n",
        "  char operationsRec;\n",
        "  int numberOfProcessors, id, to, from, tag = 1000;\n",
        "  int result, value;\n",
        "\n",
        "  MPI_Init(&argc, &argv);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcessors);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "  MPI_Status status;\n",
        "\n",
        "  switch (id)\n",
        "  {\n",
        "  case 0:\n",
        "    for (i = 0; i < SIZE; i++)\n",
        "    {\n",
        "      array[i] = i + 1;\n",
        "      printf(\"%d\\t%d\\t\", i, array[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    for (to = 1; to < numberOfProcessors; to++)\n",
        "    {\n",
        "      MPI_Send(&array, SIZE, MPI_INT, to, tag, MPI_COMM_WORLD);\n",
        "      MPI_Send(&operations[to - 1], 1, MPI_CHAR, to, tag, MPI_COMM_WORLD);\n",
        "    }\n",
        "    break;\n",
        "  default:\n",
        "    MPI_Recv(&array, SIZE, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);\n",
        "    MPI_Recv(&operationsRec, 1, MPI_CHAR, 0, tag, MPI_COMM_WORLD, &status);\n",
        "    switch (operationsRec)\n",
        "    {\n",
        "    case '+':\n",
        "      value = 0;\n",
        "      for (i = 0; i < SIZE; i++)\n",
        "        value += array[i];\n",
        "      break;\n",
        "    case '-':\n",
        "      value = 0;\n",
        "      for (i = 0; i < SIZE; i++)\n",
        "        value -= array[i];\n",
        "      break;\n",
        "    case '*':\n",
        "      value = 1;\n",
        "      for (i = 0; i < SIZE; i++)\n",
        "        value *= array[i];\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "    MPI_Send(&value, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);\n",
        "    MPI_Send(&operationsRec, 1, MPI_CHAR, 0, tag, MPI_COMM_WORLD);\n",
        "  for(to = 1; to < numberOfProcessors; to++) {\n",
        "      MPI_Recv(&result, 1, MPI_INT, to, tag, MPI_COMM_WORLD, &status);\n",
        "      MPI_Recv(&operationsRec, 1, MPI_CHAR, to, tag, MPI_COMM_WORLD, &status);\n",
        "      printf (\"(%c) = %d\\n\", operationsRec, result);\n",
        "    }\n",
        "\n",
        "     for(i = 0; i < SIZE; i++)\n",
        "  array[i] = i + 1;\n",
        "\n",
        "  for(i = 0; i < SIZE; i++)\n",
        "    printf(\"array[%d] = %d\\n\", i, array[i]);\n",
        "\n",
        "  for(i = 0; i < SIZE; i++)\n",
        "  {\n",
        "    sum = sum + array[i];\n",
        "    subtraction = subtraction - array[i];\n",
        "    mult = mult * array[i];\n",
        "  }\n",
        "  printf(\"MPI RESULTS:=========================================================================================================================================================\\n\\n\");\n",
        "  printf(\"Sum = %d\\n\", sum);\n",
        "  printf(\"Subtraction = %d\\n\", subtraction);\n",
        "  printf(\"Multiply = %d\\n\", mult);\n",
        "  MPI_Finalize();\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25f9fac-4e5d-4d0b-9e48-99e7181f8a73",
      "metadata": {
        "id": "c25f9fac-4e5d-4d0b-9e48-99e7181f8a73"
      },
      "source": [
        "### Run the Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b77d58de-a897-4ee6-b5b6-fa0be55cfe21",
      "metadata": {
        "id": "b77d58de-a897-4ee6-b5b6-fa0be55cfe21"
      },
      "outputs": [],
      "source": [
        "!mpicc operations.c -o operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f77896ee-490c-4c9a-9d32-22c73d9dd469",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f77896ee-490c-4c9a-9d32-22c73d9dd469",
        "outputId": "284f1163-d87c-4479-c52b-2dd57413e3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------\n",
            "There are not enough slots available in the system to satisfy the 4\n",
            "slots that were requested by the application:\n",
            "\n",
            "  ./operations\n",
            "\n",
            "Either request fewer slots for your application, or make more slots\n",
            "available for use.\n",
            "\n",
            "A \"slot\" is the Open MPI term for an allocatable unit where we can\n",
            "launch a process.  The number of slots available are defined by the\n",
            "environment in which Open MPI processes are run:\n",
            "\n",
            "  1. Hostfile, via \"slots=N\" clauses (N defaults to number of\n",
            "     processor cores if not provided)\n",
            "  2. The --host command line parameter, via a \":N\" suffix on the\n",
            "     hostname (N defaults to 1 if not provided)\n",
            "  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)\n",
            "  4. If none of a hostfile, the --host command line parameter, or an\n",
            "     RM is present, Open MPI defaults to the number of processor cores\n",
            "\n",
            "In all the above cases, if you want Open MPI to default to the number\n",
            "of hardware threads instead of the number of processor cores, use the\n",
            "--use-hwthread-cpus option.\n",
            "\n",
            "Alternatively, you can use the --oversubscribe option to ignore the\n",
            "number of available slots when deciding the number of processes to\n",
            "launch.\n",
            "--------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!mpirun --allow-run-as-root -np 4 ./operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be567e12",
      "metadata": {
        "id": "be567e12"
      },
      "source": [
        "## `Algebraic Function`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fe75ce",
      "metadata": {
        "id": "33fe75ce"
      },
      "source": [
        "The idea of this Hands-on is to make an algorithm that uses the\n",
        "`MPI_Recv` and `MPI_Send` routines in the Master-Worker Paradigm in such\n",
        "a way that in the sequential code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c0f91e78-8c2a-4ce1-8d6b-90a50ca1401b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0f91e78-8c2a-4ce1-8d6b-90a50ca1401b",
        "outputId": "0a5b2949-51bb-4e39-f1c4-fcd6b8b6c902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting function.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile function.c\n",
        "#include <stdio.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  double coef[4], result[4] = {0}, total = 0, x = 10, received_result;\n",
        "  int numberOfProcessors, id, index, i, to, from, tag = 1000, received_index;\n",
        "\n",
        "  // Initialize coef values\n",
        "  for (i = 1; i <= 4; i++)\n",
        "  {\n",
        "    coef[i - 1] = i;\n",
        "  }\n",
        "\n",
        "  MPI_Init(&argc, &argv);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcessors);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "  MPI_Status status;\n",
        "\n",
        "  switch (id)\n",
        "  {\n",
        "  case 0: // Master\n",
        "    // Send x to all workers\n",
        "    for (to = 1; to < numberOfProcessors; to++)\n",
        "    {\n",
        "      MPI_Send(&x, 1, MPI_DOUBLE, to, tag, MPI_COMM_WORLD);\n",
        "    }\n",
        "    break;\n",
        "\n",
        "  default: // Workers\n",
        "    // Receive x from master\n",
        "    MPI_Recv(&x, 1, MPI_DOUBLE, 0, tag, MPI_COMM_WORLD, &status);\n",
        "    index = 3 - id;  // Calculate indexed value based on id\n",
        "    result[index] = coef[index];  // Initialize result with corresponding coef value\n",
        "\n",
        "    // Multiply result by x id times\n",
        "    for (i = 1; i <= id; i++)\n",
        "    {\n",
        "      result[index] *= x;\n",
        "    }\n",
        "\n",
        "    // If id == 1, add coef[3]\n",
        "    if (id == 1)\n",
        "    {\n",
        "      result[index] += coef[3];\n",
        "    }\n",
        "\n",
        "    break;\n",
        "  }\n",
        "\n",
        "  // Send result and index back to the Master\n",
        "  MPI_Send(&result[index], 1, MPI_DOUBLE, 0, tag, MPI_COMM_WORLD);\n",
        "  MPI_Send(&index, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);\n",
        "\n",
        "  if (id == 0)\n",
        "  {\n",
        "    // Master receives results from Workers\n",
        "    for (from = 1; from < numberOfProcessors; from++)\n",
        "    {\n",
        "      MPI_Recv(&received_result, 1, MPI_DOUBLE, from, tag, MPI_COMM_WORLD, &status);\n",
        "      MPI_Recv(&received_index, 1, MPI_INT, from, tag, MPI_COMM_WORLD, &status);\n",
        "      result[received_index] = received_result;\n",
        "      printf(\"(%d) = %lf\\n\", received_index, result[received_index]);\n",
        "      total += received_result;\n",
        "    }\n",
        "\n",
        "    // Print total sum of results\n",
        "    if (total > 0)\n",
        "      printf(\"Total: %.5lf\\n\", total);\n",
        "  }\n",
        "\n",
        "  MPI_Finalize();\n",
        "  return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf967504-a3fd-48af-8f06-f1806b156548",
      "metadata": {
        "id": "bf967504-a3fd-48af-8f06-f1806b156548"
      },
      "source": [
        "### Run the Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "fcf9f267-0e70-45f0-9353-cf632a478cbc",
      "metadata": {
        "id": "fcf9f267-0e70-45f0-9353-cf632a478cbc"
      },
      "outputs": [],
      "source": [
        "!mpicc function.c -o function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "1805d761-a421-4783-87c0-61ca5f134ed2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1805d761-a421-4783-87c0-61ca5f134ed2",
        "outputId": "fbe87dde-d10f-407f-a7ff-d0c0eb01e98f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2) = 34.000000\n",
            "(1) = 200.000000\n",
            "(0) = 1000.000000\n",
            "Total: 1234.00000\n"
          ]
        }
      ],
      "source": [
        "!mpirun --allow-run-as-root -np 4 ./function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lchI5eqfCngc",
      "metadata": {
        "id": "lchI5eqfCngc"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d9I6on_NCiFC",
      "metadata": {
        "id": "d9I6on_NCiFC"
      },
      "source": [
        "OUTPUT:\n",
        "(2) = 34.000000\n",
        "(1) = 200.000000\n",
        "(0) = 1000.000000\n",
        "Total: 1234.00000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c283708-51ca-42ff-b74b-0bfa30520332",
      "metadata": {
        "id": "4c283708-51ca-42ff-b74b-0bfa30520332",
        "tags": []
      },
      "source": [
        "## `Tridiagonal Matrix`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "57f34242-b01c-4a4d-a20c-3b285fd2f041",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57f34242-b01c-4a4d-a20c-3b285fd2f041",
        "outputId": "8a072d93-43f8-4ede-88e0-0af163706dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tridiagonal.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile tridiagonal.c\n",
        "#include <stdio.h>\n",
        "#define ORDER 4\n",
        "#include <mpi.h>\n",
        "\n",
        "void printMatrix(int m[][ORDER])\n",
        "{\n",
        "  int i, j;\n",
        "  for (i = 0; i < ORDER; i++)\n",
        "  {\n",
        "    printf(\"| \");\n",
        "    for (j = 0; j < ORDER; j++)\n",
        "    {\n",
        "      printf(\"%3d \", m[i][j]);\n",
        "    }\n",
        "    printf(\"|\\n\");\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  int k[3] = {100, 200, 300};\n",
        "  int matrix[ORDER][ORDER] = {0}, received_matrix[ORDER][ORDER], i, j;\n",
        "  int numberOfProcessors, id, to, from, tag = 1000;\n",
        "  MPI_Init(&argc, &argv);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcessors);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "  MPI_Status status;\n",
        "\n",
        "  if (id == 0)\n",
        "  {\n",
        "    // Process 0 sends the initial matrix to the other processes\n",
        "    for (to = 1; to < numberOfProcessors; to++)\n",
        "    {\n",
        "      MPI_Send(&matrix, ORDER * ORDER, MPI_INT, to, tag, MPI_COMM_WORLD);\n",
        "    }\n",
        "\n",
        "    // Process 0 receives the submatrices from the other processes\n",
        "    for (from = 1; from < numberOfProcessors; from++)\n",
        "    {\n",
        "      MPI_Recv(&received_matrix, ORDER * ORDER, MPI_INT, from, tag, MPI_COMM_WORLD, &status);\n",
        "\n",
        "      // Combine the received submatrix into the final matrix\n",
        "      for (i = 0; i < ORDER; i++)\n",
        "      {\n",
        "        for (j = 0; j < ORDER; j++)\n",
        "        {\n",
        "          if (received_matrix[i][j] != 0) // Avoid overwriting existing values\n",
        "          {\n",
        "            matrix[i][j] = received_matrix[i][j];\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Print the final matrix in process 0\n",
        "    printf(\"Final matrix assembled by process 0:\\n\");\n",
        "    printMatrix(matrix);\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    // Processes 1, 2, and 3 receive the initial matrix and perform their tasks\n",
        "    MPI_Recv(&matrix, ORDER * ORDER, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);\n",
        "\n",
        "    switch (id)\n",
        "    {\n",
        "    case 1:\n",
        "      for (i = 0; i < ORDER; i++)\n",
        "        for (j = 0; j < ORDER; j++)\n",
        "          if (i == j)\n",
        "            matrix[i][j] = i + j + 1 + k[0]; // Main diagonal\n",
        "      break;\n",
        "    case 2:\n",
        "      for (i = 0; i < ORDER; i++)\n",
        "        for (j = 0; j < ORDER; j++)\n",
        "          if (i == (j + 1))\n",
        "          {\n",
        "            matrix[i][j] = i + j + 1 + k[1];    // Subdiagonal\n",
        "            matrix[j][i] = matrix[i][j] + k[2]; // Superdiagonal\n",
        "          }\n",
        "      break;\n",
        "    case 3:\n",
        "      for (i = 0; i < ORDER; i++)\n",
        "      {\n",
        "        for (j = 0; j < ORDER; j++)\n",
        "        {\n",
        "          if (i < j - 1 || i > j + 1)\n",
        "          {\n",
        "            matrix[i][j] = 0; // Zero outside the super and subdiagonal\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      break;\n",
        "    }\n",
        "    printf(\"Process %d - Matrix after the task:\\n\", id);\n",
        "    printMatrix(matrix);\n",
        "\n",
        "    // Processes 1, 2, and 3 send the submatrices back to process 0\n",
        "    MPI_Send(&matrix, ORDER * ORDER, MPI_INT, 0, tag, MPI_COMM_WORLD);\n",
        "  }\n",
        "\n",
        "  MPI_Finalize();\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "516d8529-8213-446f-a518-e2f6a6124aba",
      "metadata": {
        "id": "516d8529-8213-446f-a518-e2f6a6124aba"
      },
      "source": [
        "### Run the Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "fdb67c90-8caa-4f42-9ec5-174c5add5ed1",
      "metadata": {
        "id": "fdb67c90-8caa-4f42-9ec5-174c5add5ed1"
      },
      "outputs": [],
      "source": [
        "!mpicc tridiagonal.c -o tridiagonal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "30e01571-7a54-4eab-87b7-0b49e539e4b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30e01571-7a54-4eab-87b7-0b49e539e4b4",
        "outputId": "d6159c7b-e8a7-4764-e10d-ea645866e3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Process 3 - Matrix after the task:\n",
            "|   0   0   0   0 |\n",
            "|   0   0   0   0 |\n",
            "|   0   0   0   0 |\n",
            "|   0   0   0   0 |\n",
            "\n",
            "Final matrix assembled by process 0:\n",
            "| 101 502   0   0 |\n",
            "| 202 103 504   0 |\n",
            "|   0 204 105 506 |\n",
            "|   0   0 206 107 |\n",
            "\n",
            "Process 1 - Matrix after the task:\n",
            "| 101   0   0   0 |\n",
            "|   0 103   0   0 |\n",
            "|   0   0 105   0 |\n",
            "|   0   0   0 107 |\n",
            "\n",
            "Process 2 - Matrix after the task:\n",
            "|   0 502   0   0 |\n",
            "| 202   0 504   0 |\n",
            "|   0 204   0 506 |\n",
            "|   0   0 206   0 |\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mpirun --allow-run-as-root -np 4 ./tridiagonal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad055c0c-b24d-4d0f-8b8c-3093b0c601b5",
      "metadata": {
        "id": "ad055c0c-b24d-4d0f-8b8c-3093b0c601b5",
        "tags": []
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5fdf090-9dd3-491b-947f-81af7b8af818",
      "metadata": {
        "id": "a5fdf090-9dd3-491b-947f-81af7b8af818"
      },
      "source": [
        "M. Boratto. Hands-On Supercomputing with Parallel Computing. Available: https://github.com/muriloboratto/Hands-On-Supercomputing-with-Parallel-Computing. 2022."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a02e6bc-84d6-44d4-bb66-2a98941d08d0",
      "metadata": {
        "id": "6a02e6bc-84d6-44d4-bb66-2a98941d08d0"
      },
      "source": [
        "B. Chapman, G. Jost and R. Pas. Using OpenMP: Portable Shared Memory Parallel Programming. The MIT Press, 2007, USA."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
